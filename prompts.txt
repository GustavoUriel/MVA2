SECTION: Project Overview & Technical Architecture
  Create a world-class, enterprise-grade Flask web application for biomedical research that provides a comprehensive pipeline for multivariate analysis of multiple myeloma patients. The application processes patient data, taxonomic microbiome data, and treatment outcomes using advanced statistical methods and machine learning techniques.

  Technical Stack & Architecture:
  - Backend: Flask with Flask-RESTX for API documentation, SQLAlchemy ORM with Alembic migrations
  - Frontend: Bootstrap 5, jQuery, Chart.js/Plotly.js for visualizations, DataTables for grid functionality
  - Database: PostgreSQL (production) / SQLite (development) with connection pooling
  - Security: Flask-Security-Too, OAuth2.0 (Google), CSRF protection, input validation, rate limiting
  - Testing: pytest, pytest-flask, coverage.py, Selenium for E2E testing
  - Deployment: Docker containerization, Redis for caching, Celery for background tasks
  - Monitoring: Application logging, error tracking, performance monitoring
  - Data Processing: pandas, numpy, scikit-learn, lifelines, scipy, statsmodels

SECTION: Security & Compliance Requirements
  CRITICAL SECURITY FEATURES (Mandatory Implementation):
  1. Authentication & Authorization:
     - Multi-factor authentication (MFA) support
     - OAuth2.0 with Google, Microsoft Azure AD integration
     - Role-based access control (RBAC): Admin, Researcher, Viewer roles
     - Session management with secure cookies, automatic timeout
     - Account lockout after failed login attempts
     - Password complexity requirements and rotation policies

  2. Data Protection & Privacy:
     - End-to-end encryption for sensitive patient data
     - Data anonymization/pseudonymization capabilities
     - HIPAA compliance features (audit trails, access logs)
     - GDPR compliance (data deletion, export capabilities)
     - File upload validation and virus scanning
     - Secure file storage with access controls

  3. Application Security:
     - Input validation and sanitization (prevent SQL injection, XSS)
     - CSRF protection on all forms
     - Content Security Policy (CSP) headers
     - Rate limiting to prevent DoS attacks
     - API security with JWT tokens and request signing
     - Secure HTTP headers (HSTS, X-Frame-Options, etc.)

  4. Infrastructure Security:
     - Environment-based configuration management
     - Secrets management (Azure Key Vault, AWS Secrets Manager)
     - Database encryption at rest and in transit
     - Backup encryption and secure storage
     - Network security and firewall configurations

SECTION: Testing & Quality Assurance Strategy
  COMPREHENSIVE TESTING FRAMEWORK:
  1. Unit Testing (95%+ coverage target):
     - All business logic functions
     - Data processing algorithms
     - Statistical analysis methods
     - Database operations

  2. Integration Testing:
     - API endpoint testing
     - Database integration tests
     - External service integrations (OAuth, file storage)
     - Data pipeline end-to-end testing

  3. End-to-End Testing:
     - User workflow automation with Selenium
     - Cross-browser compatibility testing
     - Mobile responsiveness testing
     - Performance testing under load

  4. Security Testing:
     - Penetration testing protocols
     - Vulnerability scanning integration
     - Authentication/authorization testing
     - Data encryption validation

  5. Performance Testing:
     - Load testing with multiple concurrent users
     - Stress testing for large datasets
     - Memory usage and leak detection
     - Database query optimization validation

SECTION: User Interface & Experience Design
  1. Welcome page and login
    It has to have a welcome page with description of the web app and login options.
    After login it has to show many controls and information. Some of them are: 
    1.a. A square with infomation of the sample: 
      count, media of duration variable, how many got to the event as percentage and as count, how many are undefined and how many havent arrived to the event, and you can add whatever other info you consider relevant.
    1.b. A button to ingest a new data set 
      with the option to ingest from excel file or from csv files (or pasting the csv file as text into a text field) and identify which tables is from the three tables that I will mention later on.
    1.c. A square that says load current data
      and has three button in it to load data from a excel file in /current , that has the three tables in its sheets. Needs to identify for each sheet which table is, or if none. And if is a table, load into the database. When pressing buton "load patients data" loads only the table patients, when pressing in load taxos loads only taxinomies table, and when pressing load brackens results loads only bracken table.
    1.d. A button to go to the data showing and preprocessing module/page.
      It shoul be grayed until the three databases are loaded, from the current folder or from the ingest data button.

  2. Data showing and preprocessing module.
    There it has many controls: 
    2.a. A square with infomation of the sample: 
      count, media of duration variable, how many got to the event as percentage and as count, how many are undefined and how many havent arrived to the event, and you can add whatever other info you consider relevant. Updated when the table below is modified.
    2.b. Next to that square it has to have a button to save the resulting view
      (and the generated results if it was already calculated) that includes the edited/filtered/sorted dataset from the table and all the parameters on all the controls(2.c, 2.d, 2.e, 2.f, 2.g) , for later use in the user's data library. And a combobox to load any saved view. And in that combo box, next to each view, a button to delete it (with confirmation).
    2.c. A set of on/off buttons to define if add sets of columns
      as antibiotics, antifungals, antivirals, demographics, disease_characteristics, FISH indicators, comorbidities, and other sets of data. That data groups are in a list in config.py under the comment # column_group, that is already provided in the project. But you can modify that if you think that is necessary. Just modify them to the config.py file.
    
    2.c.1. Advanced Grouping Strategy Selector
      Below the column set selectors (2.c), add a single-selection option group with the following choices:
      - None (default): Perform standard multivariate analysis on all selected variables together
      - FISH Indicators: Apply hybrid grouping strategy specifically for cytogenetic data
      - Disease Characteristics: Group clinical and laboratory parameters by biological pathways
      - Demographics: Group patient characteristics by risk stratification categories
      - Genomic Markers: Group molecular markers by functional pathways and mutation types
      - Laboratory Values: Group lab results by organ system and prognostic significance
      - Treatment Response: Group treatment variables by response patterns and timing
      
      Each selection triggers a specialized grouping approach:
      
      FISH Indicators Grouping Strategy:
      - Biology-driven groups: gains vs losses, per-chromosome (chr1, chr3, chr5, chr7, chr9, chr11, chr13, chr15, chr17, chr19, chr21), known high-risk sets (del17p, t(4;14), t(14;16), t(14;20), 1q+, del1p32), complex vs simple abnormalities
      - Data-driven supplemental groups: correlation-based clustering of FISH flags, PCA components on FISH matrix, count-based groups (number of abnormalities per patient)
      - Rare event pooling: flags with <5% prevalence grouped as "rare_FISH_abnormalities"
      - Clinical relevance groups: standard-risk, intermediate-risk, high-risk, ultra-high-risk based on IMWG criteria
      
      Disease Characteristics Grouping Strategy:
      - Immunoglobulin profile: IgG, IgA, biclonal patterns, light chain restriction
      - Disease staging: ISS, R-ISS, β2-microglobulin, albumin, creatinine
      - Molecular risk: IGH rearrangements, high-risk mutations, ultra-high-risk mutations
      - Functional assessment: IMWG high-risk, functional high-risk categories
      
      Demographics Grouping Strategy:
      - Age stratification: <65 years, 65-75 years, >75 years
      - Physical characteristics: BMI categories, smoking status impact
      - Ethnic and racial risk factors: population-specific risk patterns
      
      Genomic Markers Grouping Strategy:
      - Tumor suppressor pathway: TP53, RB1 deletions and mutations
      - Oncogene pathway: MYC rearrangements, cyclin dysregulation
      - Cell cycle regulation: Cyclin D1, D2, D3 expression patterns
      - Transcription factor alterations: MAF family rearrangements
      
      Laboratory Values Grouping Strategy:
      - Kidney function: creatinine, β2-microglobulin
      - Liver function: albumin, LDH
      - Hematologic parameters: hemoglobin, platelet count, neutrophil count, lymphocyte count
      - Inflammatory markers: LDH, β2-microglobulin elevation patterns
      
      Treatment Response Grouping Strategy:
      - Induction therapy response patterns
      - Transplant-related factors: conditioning regimen intensity, engraftment timing
      - Post-transplant complications: infections, GVHD, organ toxicity
      - Long-term outcomes: relapse patterns, survival metrics
      
    2.c.2. Grouping Strategy Information Panel
      Display contextual help text explaining the medical and technical advantages of each grouping strategy:
      
      "None" Selection Info:
      "Standard multivariate analysis treating all variables independently. Advantages: Preserves individual variable effects, maintains statistical power for common variables, suitable when sample size exceeds 10 events per variable. Disadvantages: May suffer from multiple testing burden, unstable estimates for rare events, difficulty interpreting complex interactions."
      
      "FISH Indicators" Selection Info:
      "Cytogenetic-focused analysis grouping chromosomal abnormalities by biological significance. Medical advantages: Reflects known pathogenic pathways, aligns with clinical risk stratification, improves power for rare abnormalities. Technical advantages: Reduces dimensionality while preserving biological relevance, stabilizes estimates through hierarchical modeling, enables pathway-level interpretation."
      
      "Disease Characteristics" Selection Info:
      "Clinical parameter analysis grouped by pathophysiological systems. Medical advantages: Mirrors clinical assessment patterns, identifies organ system-specific risk factors, facilitates clinical translation. Technical advantages: Natural correlation structure reduces multicollinearity, improves model interpretability, enables system-level risk scoring."
      
      "Demographics" Selection Info:
      "Patient characteristic analysis stratified by established risk categories. Medical advantages: Identifies population-specific risk patterns, supports personalized medicine approaches, aligns with clinical guidelines. Technical advantages: Reduces confounding through stratification, improves generalizability across populations, enables subgroup-specific modeling."
      
      "Genomic Markers" Selection Info:
      "Molecular marker analysis grouped by functional pathways. Medical advantages: Reflects underlying biology of disease progression, identifies targetable pathways, supports precision medicine. Technical advantages: Pathway-level analysis increases statistical power, reduces noise from individual marker variability, enables functional interpretation."
      
      "Laboratory Values" Selection Info:
      "Laboratory parameter analysis grouped by organ system function. Medical advantages: Identifies organ-specific risk factors, supports monitoring strategies, reflects disease impact patterns. Technical advantages: Natural physiological correlation structure, improved clinical interpretability, enables organ system risk scoring."
      
      "Treatment Response" Selection Info:
      "Treatment variable analysis grouped by response patterns and timing. Medical advantages: Identifies optimal treatment sequences, supports clinical decision-making, reflects treatment biology. Technical advantages: Temporal correlation structure, improved power for treatment comparisons, enables personalized treatment algorithms."
    2.d. Advanced Microbiome Analysis Integration
      CRITICAL for microbiome research publications:
      A set of option buttons to select which set of bracken use (pre, during, pos, delta... or none). 
      
      Enhanced Microbiome Processing:
      - Alpha diversity metrics (Shannon, Simpson, Chao1, observed species) with statistical testing
      - Beta diversity analysis (Bray-Curtis, Jaccard, weighted/unweighted UniFrac) with PERMANOVA
      - Differential abundance testing (DESeq2, EdgeR, ANCOM-II, ALDEx2) with FDR correction
      - Compositional data analysis using CLR transformation and centered log-ratio
      - Microbiome-specific survival analysis (MiRKAT, OMiSA, MiAMi)
      - Functional pathway prediction (PICRUSt2, Tax4Fun2) and pathway survival analysis
      - Machine learning integration (Random Forest, SVM) for microbiome biomarker discovery
      - Longitudinal microbiome analysis for temporal changes
      - Microbiome-clinical variable interaction analysis
      - Zero-inflation handling and rarefaction curve analysis
      
      If one is selected, enhanced threshold controls:
      - Prevalence filtering: minimum presence across samples (5-20% recommended)
      - Abundance filtering: minimum relative abundance (0.01-0.1% recommended)  
      - Variance filtering: minimum coefficient of variation
      - Advanced normalization options: TSS, CSS, TMM, DESeq2 size factors, CLR
      - Batch effect correction for multi-center studies 
    2.e. Two slider control To define the scope of the data analysis
      (on the duration variable) that goes from 10% to 50% and are linked, one for the top % of the sample and the other for the bottom % of the table, so it takes both edges for data analysis and don't mess with the middle data. It has a check box that says that the sliding controls are linked and move together, but if unchecked it can be moved separately.
    2.f. Data processing parameters: 
      a combo box to select the method (cox, rmst, and add any other that you consider that can be usefull). And controls to the parameters to the selected method (that change when changing the selected method.
    2.g. Clustering parameters
      like random seed for clustering, max num of clusters, and any other parameter that you consider it relevant. 
    2.h. A button to process the data
      with an estimate time to get the results that it calculates from previous processing, considering variations of the data size, the sets of columns to consider, and anything that you consider relevant to make that estimation accurate. And a smaller cancel button to cancel the data processing. That button goes to the results page after the calculation is finised. Until that it shows a clock cursor and locks all the controls. Only permits scrolling in the table and the cancel button. If the data has records with no consistence in the duration or the event fields, pop up an alert saying it and saying that you are going to fix that and explain how, and have a fix them button, a just discard the invalid data button, and a cancel button. Do as selected.
    2.i. And below that a golge sheet style table
      to see the whole patients table with the selected groups of data columns, and be able to remove rows, edit the data, remove cols, filter the data, rearrange the cols, and a function to find and replace in the data.
    The results page opens in a new tab when pressing the button in 2.h.

  3. Results page
    On the results page it shows:
    3.a. A square with infomation of the sample: 
      count, media of duration variable, how many got to the event as percentage and as count, how many are undefined and how many havent arrived to the event, and you can add whatever other info you consider relevant. Updated when the table below is modified.
    3.b. A summary of all the parameters from previous page
    3.c. Advanced Results Display for Grouping Strategies
      When a grouping strategy other than "None" is selected, the results page displays:
      
      3.c.1. Tabbed Results Interface
        - Overview Tab: Summary of all group analyses with comparative metrics
        - Individual Group Tabs: Separate tab for each identified group showing detailed analysis
        - Cross-Group Comparison Tab: Statistical comparison between groups with effect size differences
        - Integrated Results Tab: Combined interpretation and clinical recommendations
      
      3.c.2. Group-Specific Analysis Results
        Each group tab contains:
        - Group composition and rationale
        - Univariate analysis results for group variables
        - Multivariate analysis within the group
        - Group-level risk score and prognostic value
        - Variable importance ranking within group
        - Interaction effects within group
        - Group-specific survival curves and forest plots
      
      3.c.3. Cross-Group Comparative Analysis
        - Effect size comparison across groups (Cohen's d, hazard ratio differences)
        - Statistical significance testing between groups (interaction tests)
        - Model performance comparison (C-index, AIC, BIC for each group model)
        - Hierarchical analysis combining group effects
        - Group-level meta-analysis when appropriate
        - Clinical relevance ranking of groups
      
      3.c.4. Enhanced Reporting Capabilities
        - Individual group reports: Detailed analysis for each group with methodology, results, and clinical interpretation
        - Comprehensive integrated report: Complete analysis including all groups, comparative results, and unified clinical recommendations
        - Executive summary: High-level findings and actionable insights for clinical decision-making
        - Technical appendix: Statistical methodology, model validation, and sensitivity analyses
    
    3.d. Publication-Ready Report Generation
      CRITICAL for scientific publication - all reports must include:
      - STROBE/CONSORT checklist compliance for observational studies
      - Detailed methodology section with statistical software versions
      - Complete statistical analysis plan documentation
      - Sensitivity analysis results and robustness testing
      - Missing data handling strategy and impact assessment
      - Model assumptions verification and diagnostic results
      - Confidence intervals for all effect estimates
      - P-value adjustment methods for multiple comparisons
      - Clinical significance interpretation alongside statistical significance
      - Limitations section with potential sources of bias
      - Data availability statement and code reproducibility information
      - Author-ready figures with publication-quality formatting (300+ DPI)
      - Complete reference list for all statistical methods used
      
      Enhanced download options based on analysis type:
      - Standard Report: Traditional scientific paper format (when "None" grouping selected)
      - Group-Specific Reports: Individual detailed reports for each group analysis
      - Comprehensive Integrated Report: Complete analysis including all groups with comparative analysis and unified conclusions
      - Executive Summary: Concise clinical decision-support document
      - Technical Documentation: Detailed methodology and validation results
      All reports maintain scientific paper style with appropriate graphs, tables, and statistical interpretations.
    3.d. Clustering information. 
      Enhanced clustering for grouped analyses:
      3.d.1. A combobox to select the criteria to select the representative variable for the cluster
        (more variate, less variate, less amount of NA, and any other criteria that you can think of). Explain when selected a criteria to select the cluster representative, what are the reasons to choose any of them.
        Additional criteria for grouped analysis:
        - Clinical relevance: Select variables with highest clinical impact within group
        - Biological pathway significance: Choose variables representing key pathway components
        - Statistical stability: Select variables with most stable estimates across bootstrap samples
        - Effect size magnitude: Choose variables with largest effect sizes within group
      3.d.2. A search box to search for any variable
        that opens the nested clusters and shows where it is clustered by highlighting it in yellow or green. Enhanced for grouped analysis to show both individual variable location and group membership.
      3.d.3. A cluster map as a nested list
        Enhanced display showing:
        - Group-level clustering (when grouping strategy is selected)
        - Within-group variable clustering
        - Cross-group variable relationships
        - Hierarchical structure with group and subgroup levels
        Use the criteria selected in 3.d.1. to sort all the variables for each cluster when showing them.
      3.d.4. A button to download the cluster tree as a pdf file well explained
        Enhanced to include group-specific clustering trees and cross-group relationship diagrams.
  3.e. Advanced Statistical Methods and Model Selection
    CRITICAL for publication quality and scientific rigor:
    
    3.e.1. Model Selection and Comparison Framework
      - Automated model selection using information criteria (AIC, BIC, AICc)
      - Cross-validated model comparison with statistical testing
      - Ensemble methods combining multiple models
      - Model averaging with Bayesian model averaging (BMA)
      - Variable selection methods: LASSO, Elastic Net, Adaptive LASSO, Group LASSO
      - Forward/backward stepwise selection with cross-validation
      - Stability selection for reproducible variable selection
      - Permutation-based importance testing
      
    3.e.2. Advanced Survival Analysis Methods
      - Competing risks analysis (Fine-Gray subdistribution hazards)
      - Time-varying effects models (extended Cox models)
      - Accelerated failure time (AFT) models
      - Cure models for long-term survivors
      - Landmark analysis for time-dependent predictions
      - Joint modeling of longitudinal and survival data
      - Machine learning survival methods (Random Survival Forest, DeepSurv)
      - Bayesian survival analysis with MCMC
      
    3.e.3. Missing Data Handling
      - Multiple imputation with chained equations (MICE)
      - Full information maximum likelihood (FIML)
      - Pattern mixture models for non-ignorable missingness
      - Missing data sensitivity analysis
      - Little's MCAR test and missing data pattern visualization
      - Propensity score-based imputation methods
      
    3.e.4. Causal Inference Methods
      - Propensity score matching and stratification
      - Inverse probability weighting (IPW)
      - Doubly robust estimation
      - Instrumental variable analysis
      - Regression discontinuity design
      - Mediation analysis with causal interpretation
      - Directed acyclic graphs (DAGs) for confounder identification

    Method parameters and filters
      The controls and results for the selected method. Now I'll enumerate if cox method was selected
      3.e.1 A sliding control to select the p value
        from more confidence to less confidence (I think that if p>0.5 the coefficient is not significant) It should be placed in a position relevant to the data, but can be moved by the user. It refreshes all the components below it, working only with the results that p is as the slider or less.
    3.f. A cox box graph of the top 20 relevant clusters
      (that is, that are absolutely more correlated to the duration variable)
    3.g. A cox analysis results table that shows the cox analysis result
      for all the covariates. It can be sorted by any column. Over it, a checkbox to mark if show only the top 20 clusters shown on the cox box graph, or all the clusters. 

  4. Additional UI/UX Features:
    4.a. Data Validation & Quality Control Dashboard:
      - Real-time data quality metrics display
      - Missing data visualization and handling options
      - Outlier detection and flagging system
      - Data consistency checks and warnings
      - Automated data cleaning suggestions

    4.b. Advanced Analytics Dashboard:
      - Interactive visualizations (survival curves, forest plots, heatmaps)
      - Comparative analysis tools between patient cohorts
      - Biomarker discovery interface with statistical significance indicators
      - Temporal analysis views for longitudinal data
      - Export capabilities for publication-ready figures

    4.c. Collaboration & Sharing Features:
      - Project workspace sharing with granular permissions
      - Real-time collaboration on analysis parameters
      - Comment system for results interpretation
      - Version control for analysis workflows
      - Integration with external systems (REDCap, OMOP CDM)

    4.d. Performance & Monitoring Dashboard:
      - System resource usage monitoring
      - Analysis execution time tracking
      - Queue management for background tasks
      - Error reporting and debugging interface
      - User activity and audit logs


SECTION: Internal working of the web app

  1- Regarding handling users: 
    It needs to have google authentication and user management
      with sessions isolation and multiple users simultaneity. The users have many fields for each of them, like text annotations, saved views (all its parameters as a json file), saved filtered or edited data sources (as a csv file), and saved results (as a comprehensive pdf report). Those saved are stored in storage folders and the user table has a list of dictionaries to them (type, name, file path, date of creation). A user can share any saved result to any other user that receives the save with the text (from xxx) added to the save name, where xxx is the name of the sender. If a result is not saved in any user, its file can be deleted. It would be nice to show the updated google profile photo next to the user on the top right. Also that clicking on the user it drops down a list with information and tools for the user, including managing the annotations, saves, etc. Be creative on that.

  1.b. Statistical Power and Sample Size Analysis:
    CRITICAL for scientific publication quality - the app must include:
    - Automated sample size calculation for each analysis method and grouping strategy
    - Power analysis for different effect sizes and significance levels
    - Post-hoc power analysis for completed studies
    - Minimum detectable effect size calculations
    - Group-specific power calculations when using advanced grouping
    - Interactive power analysis dashboard with visualization
    - Recommendations for optimal analysis strategy based on available sample size
    - Warning systems when sample size is insufficient for selected analysis 

  1.b. Enhanced User Management:
    - User roles and permissions (Admin, Researcher, Analyst, Viewer)
    - User activity tracking and audit logs
    - Customizable user preferences and settings
    - Multi-tenant data isolation with secure access controls
    - User onboarding and tutorial completion tracking
    - Notification system for shared content and system updates 
  2- Regarding the source data: 
    source_a. Patients table. 
      I have a table of patients
        with a Patient_ID, a duration_variable, an event_indicator and many other fields.
      I will provide the name
        of the id field, the name of the duration variable and the name of the event variable in a config.py file under the comment # field_names Check it.
    source_b. Taxonomies table.
      I have a table of taxonomies clasifications
        (taxonomy_id ASV Taxonomy Domain Phylum Class Order Family Genus Species) that might be used to filtering.  
    source_c. Bracken table.
      I have a table with brackens results
        that has a column with the taxonomy_id, and 3 columns for each patient_id in patients, named with the text in patient_id plus an posfix: '.2.4M' means the sample they took from the patient 24 months after the start of treatment, '.P' means the results of the sample they took previous to the treatment and '.E' is a sample they took 2 months after the start of the treatment. And each cell has the results of the sample of that patient at that time for that taxonomy. That is the bracken table. I want to create auxiliary tables: 

  3- Regarding the working data: 
    With this we generate different tables each time the sources are modified:
    data_group_a. 
      A group of 6 tables that generated from bracken
        that has the column patient_id and one column for each taxonomy, that has the value of: for table bracken_pre the value of the '.P' columns. for the table bracken_during the values of the '.E' columns, for the table bracken_pos the values of the '.2.4M' columns, for the table bracken_delta_01 the values of the column '.E' minus the column '.P', for the table bracken_delta_02 the values of the column '.2.4M' minus the column '.E', for the table bracken_delta_03 the values of the column '.2.4M' minus the column '.P'.
    data_group_b.
      A group of tables, one for each column groups in config.py as mentioned in 2.c.
        (as used in 2.c, with the name of the table as the name of the group.
    data_group_c. 
      The table of patients

  4- Regarding the data processing:
    CRITICAL: Model Validation and Scientific Rigor Framework
    Before any data processing, implement comprehensive validation:
    - Model assumptions testing (proportional hazards, linearity, independence)
    - Residual analysis and diagnostic plots
    - Influence diagnostics and outlier detection
    - Cross-validation with proper survival data handling (time-dependent)
    - Bootstrap validation with bias-corrected confidence intervals
    - Model calibration assessment (Brier score, calibration plots)
    - Harrell's C-index with confidence intervals and comparison tests
    - Integrated discrimination improvement (IDI) and net reclassification improvement (NRI)
    - Decision curve analysis for clinical utility assessment

    Standard Processing (when "None" grouping is selected):
    Replace all the NA and invalid values in all fields but duration
    Try to get info for the duration event if NA or invalid,
      as mentioned in the pop up in the alert in 2.h.
    Merge the tables selected in the selector in 2.c.
      with the table patients, matching patient_id field.
    If a bracken is selected, merge it also with the selected one from 2.d. 
      Use the threshold value (percentual or count) in 2.d. If count, round to 0 the bracken values that are below the count threshold. If proportion, round to 0 the ones below the percentage of the max value for that column
    Select from that table only the patients
      that are in the edges of duration as selected by the slider for percentages in 2.e.
    Do clustering using the parameters in 2.g. to all the columns.
      The parameters to select the name of the cluster is: If there is a column from table patients, or demographics, or disease characteristics, or FISH indicators, or comorbidities, or taxos. Look for it in that order, the name of the cluster is that column. If there is more than one column from the group, select the one using the criteria in 3.d.1.
    Do the selected multivariate analysis method
    Present the results
    
    Advanced Grouping Processing (when specific grouping strategy is selected):
    Follow the same initial data cleaning and merging steps as standard processing, then:
    
    4.1. Group Definition and Validation:
      - Apply the selected grouping strategy from GROUPING_STRATEGIES configuration
      - Validate group composition and check for overlapping variables
      - Calculate group-specific prevalence and missing data patterns
      - Identify rare events within groups and apply pooling strategies
      - Create group-level summary variables and counts
    
    4.2. Within-Group Analysis:
      - Perform univariate analysis for each variable within its group
      - Calculate group-specific correlation matrices and clustering
      - Apply group-appropriate missing data imputation strategies
      - Conduct group-level feature selection and dimensionality reduction
      - Generate group-specific risk scores and composite variables
    
    4.3. Group-Level Modeling:
      - Fit separate multivariate models for each group using group-appropriate methods
      - Apply hierarchical modeling when groups have natural nested structure
      - Use pathway-based constraints for biologically-defined groups
      - Implement stratified analysis for demographic and clinical groups
      - Perform temporal analysis for treatment-related groups
    
    4.4. Cross-Group Analysis:
      - Compare effect sizes and statistical significance across groups
      - Test for group-level interactions and effect modification
      - Perform meta-analysis across groups when appropriate
      - Calculate group-specific model performance metrics (C-index, AIC, BIC)
      - Assess clinical relevance and importance ranking of groups
    
    4.5. Integrated Results Generation:
      - Combine group-specific results into unified risk prediction models
      - Generate hierarchical risk stratification incorporating all groups
      - Create clinical decision algorithms based on group findings
      - Develop personalized risk scores using group-specific weights
      - Produce comprehensive interpretation with clinical recommendations

  4.b. Advanced Data Processing Pipeline:
    - Automated data validation and quality assessment
    - Machine learning-based missing data imputation
    - Robust outlier detection using multiple algorithms
    - Feature engineering and selection automation
    - Cross-validation and model validation frameworks
    - Reproducible analysis with version tracking
    - Parallel processing for large datasets
    - Real-time progress monitoring and logging

  4.c. Clinical Decision Support System:
    CRITICAL for clinical adoption and commercial viability:
    - AI-powered risk stratification with interpretable predictions
    - Treatment recommendation engine based on patient characteristics
    - Prognostic calculator with uncertainty quantification
    - Clinical alert system for high-risk patients
    - Biomarker signature validation and discovery
    - Personalized treatment response prediction
    - Integration with electronic health records (HL7 FHIR)
    - Clinical guideline compliance checking
    - Real-world evidence generation capabilities
    - Pharmacovigilance and adverse event prediction
    - Cost-effectiveness analysis integration
    - Clinical trial matching and patient selection support

  5- Regarding rules to apply in all the project.
    5.a. add comments to everything in the code
    5.b. add a logging agent and log everything in a log file
    5.c. all tables are user dependant, 
      that is that the user can load his own data and process it, while other user uses other set of data.
    5.d. Use the definitions in config.py under the comments # column_names_mapping
      to rename the columns names for the source patients table and the source taxonomy table. If it doesn't match use fuzzy method to asign each column.
    5.e. Use the definitions in config.py under the comments # identification_fields
    to recognize the columns names for the source patients table. If it doesn't match use fuzzy method to asign each column.
    5.f. For all the taxonomies handling we use the id.
      But in all the presentation on the page it needs to show the taxo name, not the id. 
      All taxos that are shown in the web app should show a popup with the taxo data when hovering on it 0.3 seconds)

  6- Regarding other auxiliary functions
    I will set in config.py 
      the expected column names for table patients and for table taxonomies. For table brackens it should have columns named as the patients id with the posfixes mentioned in 2.c. If the posfix don't match use fuzzy methods to identify them. The patient_id must match exactly.
    This project should be placed in a pythonanywhere account. Make it ready for that.
    This should be done using 
      flask, pandas, numpy, sklearn, lifelines, plotly, and any other library that you consider relevant. Use bootstrap or any other library to make the web app pretty and easy to use.
    All the code should be in a github repository
      that I can access. I will provide the repository link.
    All the controls and elements in the page
      should have a explanatory text next to it or in a popup after 1 second of hovering
    Add a page with a tutorial, 
      that is a extensive, detailed, precise explanation of everything that this webapp does in a technical but not so formal language. The target is scientifics that specializes on biomedicine and cancer treatment but don't know so much about mathmatics. Explain all the methods used to process the data, the algorithms used for clustering the methods for analysis and its differences, and how to interpret the results. It is a tutorial for the beginner to use this webapp.
      
      Enhanced Tutorial Sections for Advanced Grouping:
      
      Tutorial Section: "Understanding Analysis Strategies"
      - Standard vs. Grouped Analysis: When to use each approach and their respective advantages
      - Clinical Rationale for Grouping: How biological pathways and clinical classifications inform grouping strategies
      - Statistical Benefits: Power enhancement, multiple testing control, and effect size interpretation
      
      Tutorial Section: "FISH Indicators Analysis"
      - Cytogenetic Basics: Explanation of chromosomal abnormalities in multiple myeloma
      - High-Risk vs. Standard-Risk Patterns: Clinical significance of different FISH patterns
      - Grouping Rationale: Why chromosome-based and risk-based groupings improve analysis
      - Interpretation Guide: Understanding hazard ratios and confidence intervals for cytogenetic groups
      
      Tutorial Section: "Disease Characteristics Grouping"
      - Multiple Myeloma Staging: ISS, R-ISS, and prognostic factors
      - Laboratory Parameter Interpretation: β2-microglobulin, albumin, creatinine significance
      - Molecular Risk Stratification: IGH rearrangements and high-risk mutations
      - Clinical Application: How grouped results inform treatment decisions
      
      Tutorial Section: "Comparative Analysis Interpretation"
      - Cross-Group Comparisons: Understanding effect size differences between groups
      - Model Performance Metrics: C-index, AIC, BIC interpretation for group models
      - Clinical Relevance Ranking: How to prioritize groups for clinical decision-making
      - Integrated Risk Assessment: Combining multiple group results for personalized medicine
      
      Tutorial Section: "Report Interpretation Guide"
      - Individual Group Reports: Reading and understanding group-specific analyses
      - Comprehensive Reports: Integrating findings across multiple groups
      - Executive Summaries: Translating statistical results to clinical recommendations
      - Limitations and Caveats: Understanding the scope and limitations of grouped analyses
    Remember that each user can have his own dataset. Each user has his own instance folder. Create a folder for each logged user that is named as as the user email (replace the @ with and underscore) inside the folder /instance
    Use that folder as user folder, for each user
    Use sqlalchemy for database
    Use logging for each user. The log file should be a file named as the user email before the @, and placed in the user folder
    Use python-dotenv to load environment variables from a .env file. I will place there the oauth credentials
    Check and add anything that might be missing in the .env file and in config.py

SECTION: Performance & Scalability Requirements
  CRITICAL PERFORMANCE FEATURES FOR COMMERCIAL VIABILITY:
  1. Backend Optimization:
     - Database query optimization with proper indexing
     - Connection pooling and query caching
     - Asynchronous task processing with Celery
     - Memory-efficient data processing for large datasets (>100K patients)
     - Pagination and lazy loading for large result sets
     - GPU acceleration for machine learning computations
     - Distributed computing for large-scale analyses
     - Intelligent caching strategies for repeated analyses
     - Auto-scaling based on computational demand

  2. Frontend Performance:
     - Progressive loading of components
     - Client-side caching strategies
     - Optimized bundle sizes and asset compression
     - Virtual scrolling for large data tables (>10K rows)
     - Responsive design for mobile devices
     - WebAssembly integration for client-side computations
     - Progressive Web App (PWA) capabilities for offline use
     - Real-time collaboration features with WebSocket

  3. Infrastructure Scaling:
     - Horizontal scaling capabilities for multi-institutional deployments
     - Load balancing configuration for high availability
     - CDN integration for global static asset delivery
     - Database sharding strategies for multi-tenant architecture
     - Auto-scaling based on resource utilization
     - Kubernetes orchestration for container management
     - Edge computing for reduced latency
     - Multi-cloud deployment strategies

SECTION: Error Handling & Monitoring
  COMPREHENSIVE ERROR MANAGEMENT:
  1. Application-Level Error Handling:
     - Graceful degradation for component failures
     - User-friendly error messages with actionable guidance
     - Automatic retry mechanisms for transient failures
     - Circuit breaker pattern for external service calls
     - Rollback capabilities for failed operations

  2. Monitoring & Alerting:
     - Real-time application health monitoring
     - Performance metrics dashboards
     - Error rate tracking and alerting
     - User experience monitoring
     - Resource utilization tracking

  3. Logging & Debugging:
     - Structured logging with correlation IDs
     - Distributed tracing for complex operations
     - Debug mode with detailed error information
     - Log aggregation and analysis tools
     - Audit trail for sensitive operations

SECTION: Commercial Viability and Market Differentiation

  CRITICAL FEATURES FOR COMMERCIAL SUCCESS:
  
  1. Enterprise Integration Capabilities:
     - REDCap integration for clinical data management
     - Epic/Cerner EHR integration via HL7 FHIR
     - OMOP Common Data Model support for multi-institutional studies
     - SAS/R/Python code export for regulatory submissions
     - Clinical data warehouse connectivity
     - Laboratory information system (LIS) integration
     - Pathology system integration for tissue-based data

  2. Regulatory and Compliance Features:
     - FDA 21 CFR Part 11 electronic records compliance
     - ICH E9 statistical principles implementation
     - Good Clinical Practice (GCP) audit trail support
     - Data integrity and ALCOA+ compliance
     - Regulatory submission package generation
     - Clinical study report automation
     - Risk-based quality management integration

  3. Multi-Institutional and Consortium Support:
     - Federated learning capabilities for multi-site studies
     - Harmonized data standardization across institutions
     - Collaborative analysis workspaces
     - Secure data sharing with privacy preservation
     - Institutional review board (IRB) integration
     - Data use agreement management
     - Consortium governance and access control

  4. AI/ML and Precision Medicine Features:
     - Biomarker discovery and validation pipelines
     - Personalized risk prediction models
     - Treatment response prediction algorithms
     - Adverse event prediction and monitoring
     - Real-world evidence generation
     - Comparative effectiveness research tools
     - Health economic outcomes analysis

  5. Commercial Licensing and Monetization:
     - Tiered subscription models (academic, hospital, pharma)
     - Usage-based pricing for computational resources
     - White-label deployment options
     - API access for third-party integrations
     - Professional services and consulting offerings
     - Training and certification programs
     - Support ticket system and SLA management

SECTION: Data Management & Compliance
  ENTERPRISE DATA HANDLING:
  1. Data Lifecycle Management:
     - Automated data retention policies
     - Data archiving and purging strategies
     - Version control for datasets and analysis results
     - Data lineage tracking and provenance
     - Backup and disaster recovery procedures

  2. Compliance & Governance:
     - HIPAA compliance for healthcare data
     - GDPR compliance for European users
     - SOC 2 Type II certification readiness
     - Data classification and labeling
     - Regular compliance audits and reporting

  3. Data Quality Assurance:
     - Real-time data validation rules
     - Data profiling and quality metrics
     - Anomaly detection in incoming data
     - Data standardization and normalization
     - Quality score calculation and reporting

SECTION: API Design & Integration
  REST API SPECIFICATIONS:
  1. API Architecture:
     - RESTful API design with OpenAPI 3.0 specification
     - Versioning strategy for backward compatibility
     - Rate limiting and throttling mechanisms
     - API key management and authentication
     - Response caching and optimization

  2. External Integrations:
     - FHIR compliance for healthcare data exchange
     - Integration with common research databases (REDCap, OMOP CDM)
     - Cloud storage integration (AWS S3, Azure Blob, Google Cloud)
     - Notification services (email, Slack, Teams)
     - Export capabilities to common formats (CSV, Excel, PDF, JSON)

  3. Real-time Features:
     - WebSocket connections for live updates
     - Server-sent events for progress tracking
     - Real-time collaboration features
     - Live data streaming capabilities
     - Push notifications for important events

SECTION: Development & Deployment Strategy
  DEVOPS & CI/CD PIPELINE:
  1. Development Environment:
     - Docker containerization for consistent environments
     - Local development setup with hot reloading
     - Database migrations with Alembic
     - Environment-specific configuration management
     - Code quality tools (pre-commit hooks, linters)

  2. Testing Strategy:
     - Automated testing pipeline with pytest
     - Code coverage reporting (target: 95%+)
     - Integration testing with test databases
     - End-to-end testing with Selenium
     - Performance testing and benchmarking

  3. Deployment & Operations:
     - Blue-green deployment strategy
     - Automated deployment with GitHub Actions
     - Infrastructure as Code (Terraform/CloudFormation)
     - Container orchestration with Kubernetes
     - Monitoring and logging in production

SECTION: User Experience & Accessibility
  INCLUSIVE DESIGN PRINCIPLES:
  1. Accessibility Features:
     - WCAG 2.1 AA compliance
     - Screen reader compatibility
     - Keyboard navigation support
     - High contrast mode and dark theme
     - Internationalization (i18n) support

  2. User Experience Optimization:
     - Mobile-first responsive design
     - Progressive web app (PWA) capabilities
     - Offline functionality for critical features
     - Contextual help and onboarding
     - Customizable dashboard layouts

  3. Performance Optimization:
     - Page load times under 3 seconds
     - Optimized images and assets
     - Lazy loading of non-critical components
     - Client-side caching strategies
     - Graceful handling of slow network conditions

SECTION: Project Structure & File Organization
  RECOMMENDED PROJECT STRUCTURE:
  ```
  mva-webapp/
  ├── app/
  │   ├── __init__.py
  │   ├── models/
  │   │   ├── __init__.py
  │   │   ├── user.py
  │   │   ├── patient.py
  │   │   ├── taxonomy.py
  │   │   └── analysis.py
  │   ├── api/
  │   │   ├── __init__.py
  │   │   ├── auth.py
  │   │   ├── data.py
  │   │   └── analysis.py
  │   ├── services/
  │   │   ├── __init__.py
  │   │   ├── data_processor.py
  │   │   ├── statistical_analyzer.py
  │   │   └── clustering_service.py
  │   ├── utils/
  │   │   ├── __init__.py
  │   │   ├── validators.py
  │   │   ├── helpers.py
  │   │   └── decorators.py
  │   ├── static/
  │   │   ├── css/
  │   │   ├── js/
  │   │   └── images/
  │   └── templates/
  │       ├── base.html
  │       ├── auth/
  │       ├── dashboard/
  │       └── results/
  ├── tests/
  │   ├── unit/
  │   ├── integration/
  │   └── e2e/
  ├── migrations/
  ├── docker/
  ├── docs/
  ├── config.py
  ├── requirements.txt
  ├── Dockerfile
  ├── docker-compose.yml
  └── run.py
  ```

SECTION: Implementation Requirements & Specifications
  DETAILED IMPLEMENTATION GUIDELINES:
  
  1. Code Quality Standards:
     - Follow PEP 8 style guidelines
     - Use type hints for all functions
     - Comprehensive docstrings (Google style)
     - Modular design with clear separation of concerns
     - Design patterns: Factory, Repository, Observer
  
  2. Database Design:
     - Normalized database schema
     - Proper foreign key relationships
     - Database indexes for performance
     - Migration scripts for schema changes
     - Data archiving and cleanup procedures
  
  3. API Design Standards:
     - RESTful API endpoints
     - Consistent response formats
     - Proper HTTP status codes
     - Request/response validation
     - API rate limiting and throttling
  
  4. Frontend Development:
     - Progressive web app (PWA) features
     - Responsive design (mobile-first)
     - Accessibility compliance (WCAG 2.1)
     - Browser compatibility (modern browsers)
     - Performance optimization techniques

SECTION: Requirements & Dependencies Management
  PRODUCTION-READY REQUIREMENTS:
  
  Core Flask & Web Framework:
  ```
  Flask==2.3.3
  Flask-SQLAlchemy==3.0.5
  Flask-Migrate==4.0.5
  Flask-Login==0.6.3
  Flask-WTF==1.1.1
  Flask-Mail==0.9.1
  Flask-Security-Too==5.3.2
  Flask-RESTX==1.3.0
  Flask-Limiter==3.5.0
  ```
  
  Database & Caching:
  ```
  SQLAlchemy==2.0.21
  psycopg2-binary==2.9.7
  redis==5.0.0
  alembic==1.12.0
  ```
  
  Authentication & Security:
  ```
  authlib==1.2.1
  cryptography==41.0.4
  bcrypt==4.0.1
  PyJWT==2.8.0
  google-auth==2.23.0
  google-auth-oauthlib==1.0.0
  ```
  
  Data Processing & Analytics:
  ```
  pandas==2.1.1
  numpy==1.25.2
  scikit-learn==1.3.0
  scipy==1.11.2
  statsmodels==0.14.0
  lifelines==0.27.7
  matplotlib==3.7.2
  seaborn==0.12.2
  plotly==5.16.1
  ```
  
  Background Tasks & Monitoring:
  ```
  celery==5.3.2
  gunicorn==21.2.0
  sentry-sdk==1.32.0
  prometheus-client==0.17.1
  ```
  
  Testing & Quality Assurance:
  ```
  pytest==7.4.2
  pytest-flask==1.2.0
  pytest-cov==4.1.0
  selenium==4.12.0
  factory-boy==3.3.0
  faker==19.6.2
  ```
  
  Utilities & File Processing:
  ```
  python-dotenv==1.0.0
  openpyxl==3.1.2
  xlrd==2.0.1
  python-magic==0.4.27
  fuzzywuzzy==0.18.0
  python-levenshtein==0.21.1
  ```

SECTION: Deployment & DevOps Configuration
  DOCKER & CONTAINERIZATION:
  
  Dockerfile:
  ```dockerfile
  FROM python:3.11-slim
  
  WORKDIR /app
  
  # Install system dependencies
  RUN apt-get update && apt-get install -y \
      gcc \
      g++ \
      libpq-dev \
      && rm -rf /var/lib/apt/lists/*
  
  # Copy requirements and install Python dependencies
  COPY requirements.txt .
  RUN pip install --no-cache-dir -r requirements.txt
  
  # Copy application code
  COPY . .
  
  # Create non-root user
  RUN adduser --disabled-password --gecos '' appuser
  RUN chown -R appuser:appuser /app
  USER appuser
  
  EXPOSE 8000
  
  CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "run:app"]
  ```
  
  docker-compose.yml:
  ```yaml
  version: '3.8'
  
  services:
    web:
      build: .
      ports:
        - "8000:8000"
      environment:
        - FLASK_ENV=production
        - DATABASE_URL=postgresql://postgres:password@db:5432/mva_db
        - REDIS_URL=redis://redis:6379/0
      depends_on:
        - db
        - redis
      volumes:
        - ./uploads:/app/uploads
    
    db:
      image: postgres:15
      environment:
        - POSTGRES_DB=mva_db
        - POSTGRES_USER=postgres
        - POSTGRES_PASSWORD=password
      volumes:
        - postgres_data:/var/lib/postgresql/data
    
    redis:
      image: redis:7-alpine
      ports:
        - "6379:6379"
    
    celery:
      build: .
      command: celery -A app.celery worker --loglevel=info
      environment:
        - CELERY_BROKER_URL=redis://redis:6379/0
        - CELERY_RESULT_BACKEND=redis://redis:6379/0
      depends_on:
        - redis
        - db
  
  volumes:
    postgres_data:
  ```

SECTION: Data Validation & Quality Assurance
  COMPREHENSIVE DATA VALIDATION:
  
  1. Input Validation Rules:
     - Patient ID format validation (alphanumeric, length constraints)
     - Date format validation and range checks
     - Numerical value range validation
     - Categorical value enumeration checks
     - File format and size validation
  
  2. Data Quality Metrics:
     - Completeness score calculation
     - Consistency checks across related fields
     - Accuracy validation against reference ranges
     - Timeliness checks for temporal data
     - Uniqueness validation for patient identifiers
  
  3. Automated Data Cleaning:
     - Standardization of categorical values
     - Outlier detection and flagging
     - Missing value pattern analysis
     - Duplicate record identification
     - Data type conversion and normalization

SECTION: Regulatory Compliance & Audit Features
  HEALTHCARE DATA COMPLIANCE:
  
  1. HIPAA Compliance Features:
     - Patient data de-identification tools
     - Access control and authorization logging
     - Data breach detection and reporting
     - Secure data transmission protocols
     - Regular compliance audits and reports
  
  2. GDPR Compliance Features:
     - Right to be forgotten implementation
     - Data portability and export functions
     - Consent management system
     - Data processing activity logging
     - Privacy impact assessment tools
  
  3. FDA 21 CFR Part 11 Compliance:
     - Electronic signature validation
     - Audit trail integrity
     - System validation documentation
     - Change control procedures
     - Data integrity controls

SECTION: Advanced Analytics & Machine Learning
  CUTTING-EDGE ANALYTICAL CAPABILITIES:
  
  1. Machine Learning Pipeline:
     - Automated feature engineering
     - Model selection and hyperparameter tuning
     - Cross-validation and model evaluation
     - Ensemble methods for improved accuracy
     - Model interpretability and explainability
  
  2. Advanced Statistical Methods:
     - Bayesian survival analysis
     - Time-varying coefficient models
     - Competing risks analysis
     - Propensity score matching
     - Causal inference methods
  
  3. Microbiome-Specific Analytics:
     - Alpha and beta diversity calculations
     - Differential abundance testing
     - Functional pathway analysis
     - Metabolic network reconstruction
     - Multi-omics data integration

SECTION: Final Implementation Checklist
  PRODUCTION READINESS VERIFICATION:
  
  ✓ Security hardening complete
  ✓ Performance optimization implemented
  ✓ Comprehensive testing suite created
  ✓ Documentation and user guides written
  ✓ Deployment automation configured
  ✓ Monitoring and alerting systems active
  ✓ Compliance requirements validated
  ✓ Backup and disaster recovery tested
  ✓ Scalability architecture verified
  ✓ User acceptance testing completed